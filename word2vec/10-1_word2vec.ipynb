{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import collections\n",
    "import random\n",
    "import math \n",
    "import torch.utils.data \n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0+cu113\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#sentences:42068'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert 'ptb.train.txt' in os.listdir('/home/word2vec/data')\n",
    "with open('/home/word2vec/data/ptb.train.txt','r')as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split()for st in lines]\n",
    "'#sentences:%d'%(len(raw_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32481"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x:x[1]>=5,counter.items()))\n",
    "counter['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_taken = [tk for tk,_ in counter.items()]\n",
    "token_to_idx = {tk:idx for idx,tk in enumerate(idx_to_taken)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx]for st in raw_dataset]\n",
    "num_tokens = sum([len(st)for st in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32481"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter[idx_to_taken[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_taken[2]\n",
    "token_to_idx['N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375861\n"
     ]
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0,1)<1-math.sqrt(1e-4/counter[idx_to_taken[idx]]*num_tokens)\n",
    "sampled_data_set = [[tk for tk in st if not discard(tk)]for st in dataset]\n",
    "print(sum([len(st)for st in sampled_data_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#the: before:50770,after:2113'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_count(token):\n",
    "    return '#%s: before:%d,after:%d'%(token,sum([st.count(token_to_idx[token])for st in dataset]),sum([st.count(token_to_idx[token])for st in sampled_data_set]))\n",
    "compare_count('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset,max_window_size):\n",
    "    centers,contexts= [],[]\n",
    "    for st in dataset:\n",
    "        if len(st)<2:\n",
    "            continue\n",
    "        centers +=st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1,max_window_size)\n",
    "            indices = list(range(max(0,center_i-window_size),min(len(st),center_i+window_size+1)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx]for idx in indices])\n",
    "    return centers,contexts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "center: 0 has contexts: [1, 2]\n",
      "center: 1 has contexts: [0, 2]\n",
      "center: 2 has contexts: [0, 1, 3, 4]\n",
      "center: 3 has contexts: [2, 4]\n",
      "center: 4 has contexts: [2, 3, 5, 6]\n",
      "center: 5 has contexts: [3, 4, 6]\n",
      "center: 6 has contexts: [4, 5]\n",
      "center: 7 has contexts: [8, 9]\n",
      "center: 8 has contexts: [7, 9]\n",
      "center: 9 has contexts: [7, 8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)),list(range(7,10))]\n",
    "print('dataset',tiny_dataset)\n",
    "for centers,contexts in zip(*get_centers_and_contexts(tiny_dataset,2)):\n",
    "    print('center:',centers,'has contexts:',contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374933 374933\n",
      "9858\n",
      "9858\n"
     ]
    }
   ],
   "source": [
    "all_centers,all_contexts = get_centers_and_contexts(sampled_data_set,5)\n",
    "print(len(all_centers),len(all_contexts))\n",
    "print(len(idx_to_taken))\n",
    "def get_negatives(all_contexts,sampling_weight,K):\n",
    "    all_negatives,neg_candidates,i= [],[],0\n",
    "    populations = list(range(len(sampling_weight)))\n",
    "    print(len(populations))\n",
    "    for contexts in all_contexts:\n",
    "        negatives= []\n",
    "        while len(negatives)<len(contexts)*K:\n",
    "            if i==len(neg_candidates):\n",
    "                i=0\n",
    "                neg_candidates  = random.choices(populations,sampling_weight,k = int(1e5))\n",
    "            neg =  neg_candidates[i]\n",
    "            i = i+1\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "sampling_weight = [counter[w]**0.75 for w in idx_to_taken]\n",
    "all_negatives =  get_negatives(all_contexts,sampling_weight,K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDateset(torch.utils.data.Dataset):\n",
    "    def __init__(self,centers,contexts,negatives):\n",
    "        assert len(centers)==len(contexts)==len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    def __getitem__(self,index):\n",
    "        return (self.centers[index],self.contexts[index],self.negatives[index])\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len = max(len(c)+len(n) for _,c,n in data)\n",
    "    centers,contexts_negatives,masks,labels =[],[],[],[]\n",
    "    for center,context,negative in data:\n",
    "        cur_len = len(context)+len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [(context+negative+[0]*(max_len-cur_len))]\n",
    "        masks += [[1]*cur_len+[0]*(max_len-cur_len)]\n",
    "        labels += [[1]*len(contexts)+[0]*(max_len-len(contexts))]\n",
    "    return torch.tensor(centers).view(-1,1),torch.tensor(contexts_negatives),torch.tensor(masks),torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1])\n",
      "torch.Size([512, 60])\n",
      "torch.Size([512, 60])\n",
      "torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size=512\n",
    "num_workers = 0 \n",
    "dataset = MyDateset(all_centers,all_contexts,all_negatives)\n",
    "data_iter = torch.utils.data.DataLoader(dataset,batch_size=batch_size,num_workers=num_workers,shuffle=True,collate_fn=batchify)\n",
    "for batch in data_iter:\n",
    "    for name,data in zip(['centers','contexts_negative','mask','labels'],batch):\n",
    "        print(data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.7334,  1.4351, -1.0346, -0.6599],\n",
       "         [-0.2932, -1.2891, -0.1360, -0.2086],\n",
       "         [-0.7851, -0.4692, -1.1269, -1.7478]],\n",
       "\n",
       "        [[ 0.9005, -2.1161,  1.5750, -0.2610],\n",
       "         [-0.8368,  1.4180,  0.9319,  1.8862],\n",
       "         [-0.2481, -0.4609, -0.2558,  0.6948]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20,embedding_dim=4)\n",
    "x = torch.tensor([[1,2,3],[4,5,6]],dtype=torch.long)\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4., 4., 4., 4., 4., 4.]],\n",
       "\n",
       "        [[4., 4., 4., 4., 4., 4.]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((2,1,4))\n",
    "y = torch.ones((2,4,6))\n",
    "torch.bmm(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center,contexts_and_negatives,embed_v,embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v,u.permute(0,2,1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss,self).__init__()\n",
    "    def forward(self,inputs,targets,mask=None):\n",
    "        targets = targets.float()\n",
    "        mask=mask.float()\n",
    "        inputs = inputs.float()\n",
    "        res = torch.nn.functional.binary_cross_entropy_with_logits(inputs,targets,reduction=\"none\",weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "        \n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8740, 1.2100])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred =  torch.tensor([[1.5,0.3,-1,2],[1.1,-0.6,2.2,0.4]])\n",
    "labels = torch.tensor([[1,0,0,0],[1,1,0,0]])\n",
    "mask = torch.tensor([[1,1,1,1],[1,1,1,0]])\n",
    "loss(pred,labels,mask)*mask.shape[1]/mask.float().sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0049)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "loss = -labels*np.log(sigmoid(pred)) - (1-labels)*np.log(1-sigmoid(pred))\n",
    "loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740\n"
     ]
    }
   ],
   "source": [
    "def sigmd(x):\n",
    "    return -math.log(1/(1+math.exp(-x)))\n",
    "print('%.4f'%((sigmd(1.5)+sigmd(-0.3)+sigmd(1)+sigmd(-2))/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9858\n"
     ]
    }
   ],
   "source": [
    "print(len(idx_to_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=len(idx_to_taken),embedding_dim=embed_size),\n",
    "    nn.Embedding(num_embeddings=len(idx_to_taken),embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,lr,num_epochs):\n",
    "    device =torch.device('cuda' if torch.cuda.is_available()else 'cpu')\n",
    "    net= net.to(device)\n",
    "    print('trian on:',device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        n = 0\n",
    "        for batch in data_iter:\n",
    "            centers,contexts_negatives,mask,labels = [b.to(device)for b in batch]\n",
    "            pred = skip_gram(centers,contexts_negatives,net[0],net[1])\n",
    "            l = (loss(pred.view(labels.shape),labels,mask)*mask.shape[1]/mask.float().sum(dim=1)).mean()\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += l.cpu().item()\n",
    "            n +=1\n",
    "        print('epoch:%d,loss:%.4f,time:%.2f'%(epoch+1,train_loss/n,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net,0.01,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=1.000:a\n",
      "cosine sim=0.375:suspected\n",
      "cosine sim=0.356:mitsubishi\n",
      "cosine sim=0.354:bugs\n",
      "cosine sim=0.351:pa\n",
      "cosine sim=0.335:disorders\n",
      "cosine sim=0.330:resource\n",
      "cosine sim=0.323:layer\n",
      "cosine sim=0.319:other\n",
      "cosine sim=0.319:estimates\n",
      "cosine sim=0.313:picket\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token,k,embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W,x)/(torch.sum(W*W,dim=1)*torch.sum(x*x)+1e-9).sqrt()\n",
    "    _,topk = torch.topk(cos,k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[:]:\n",
    "        print('cosine sim=%.3f:%s'%(cos[i],(idx_to_taken[i])))\n",
    "get_similar_tokens('a',10,net[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No devices were found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4562)\n"
     ]
    }
   ],
   "source": [
    "logit = torch.tensor([5.0, 1.0, 3.0], dtype=torch.float32)\n",
    "label = torch.tensor([1.0, 0.0, 1.0], dtype=torch.float32)\n",
    "output = torch.nn.functional.binary_cross_entropy_with_logits(logit, label)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
